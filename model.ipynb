{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef102b53-2348-490e-8257-ac32916d5277",
   "metadata": {},
   "source": [
    "# Traffic Light Autoencoder - Starter Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cb083df-4bb4-4b43-b224-d556d0ae1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, random, subprocess, sys, zipfile\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c1c8e-394d-4206-beee-803d20a3e311",
   "metadata": {},
   "source": [
    "## Step 1: Download the zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101a094-5e8c-4e79-95bf-2106177ef0d7",
   "metadata": {},
   "source": [
    "This snippet downloads the training dataset ZIP from the course server and saves it locally as training_dataset.zip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53e79d80-cc3b-4c73-ae70-ee65c4b501af",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://hadi.cs.virginia.edu:9000/download/train-dataset-hw2\"\n",
    "out = Path(\"training_dataset.zip\")\n",
    "\n",
    "with requests.get(url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with out.open(\"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7848b-0b1e-4554-b9e4-3434b7287248",
   "metadata": {},
   "source": [
    "## Step 2: Provide the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87f844be-c566-4598-85cd-9758e21fc2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f15bbd16910>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- CONFIG ----------\n",
    "DATA_ROOT  = Path(\"data\")\n",
    "ZIP_PATH   = Path(\"training_dataset.zip\")  # provided to students\n",
    "TRAIN_DIR  = DATA_ROOT / \"training_dataset\"        # unzip target\n",
    "\n",
    "IMG_SIZE   = 256 #NOT ALLOWED TO CHANGE\n",
    "GRAYSCALE  = False\n",
    "LATENT_DIM = 32\n",
    "BATCH_SIZE = 64            # drop to 32/16 if OOM\n",
    "EPOCHS     = 20\n",
    "LR         = 2e-3\n",
    "SEED       = 42\n",
    "MIN_BOX    = 8\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LAMBDA_L1  = 0.5       # Optional: use mixed loss for sharper recon (0..1) # 0 = pure MSE, 0.5 = half MSE + half L1\n",
    "random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a759f87-f10d-4cc0-a634-a507fb99fb18",
   "metadata": {},
   "source": [
    "## Step 3: Unzip the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b03e8011-2906-4ce7-8499-9b9266a103b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images already present under: data/training_dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- Unzip training dataset ----------\n",
    "def ensure_unzipped(zip_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # If folder already has images, skip\n",
    "    has_images = any(p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
    "                     for p in out_dir.rglob(\"*\"))\n",
    "    if has_images:\n",
    "        print(f\"Training images already present under: {out_dir}\")\n",
    "        return\n",
    "    if not zip_path.exists():\n",
    "        raise SystemExit(f\"❗️ Zip not found: {zip_path}\\n\"\n",
    "                         f\"Place training_dataset.zip at {zip_path} and rerun.\")\n",
    "    print(f\"Unzipping {zip_path} -> {out_dir} ...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    print(\"Unzip done.\")\n",
    "\n",
    "ensure_unzipped(ZIP_PATH, TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9ad4a-8233-47f8-a4e7-42d423bb3802",
   "metadata": {},
   "source": [
    "## Optional Step (Please READ)\n",
    "This zip file was created on MACOS so it may happen that a __MACOSX folder may be created inside data/training_dataset. If it is, please execute below code to delete that folder as it leads 2xthe number of images, which will hamper your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9943174-708c-4186-85c3-58cdb2f89ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to remove.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import stat\n",
    "\n",
    "target = Path(\"data/training_dataset/__MACOSX\")\n",
    "\n",
    "if target.exists():\n",
    "    if target.is_dir():\n",
    "        shutil.rmtree(target)\n",
    "        print(f\"Removed: {target}\")\n",
    "    else:\n",
    "        print(f\"Exists but is not a directory: {target}\")\n",
    "else:\n",
    "    print(\"Nothing to remove.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815db0e-3078-4f06-a1ec-004a77a842f9",
   "metadata": {},
   "source": [
    "## Step 4: Collect all training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7359354a-fb5d-4b76-8ae7-9d6cab77a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val (ALL training_dataset/ images): train=12630  val=1404\n"
     ]
    }
   ],
   "source": [
    "# ---------- Collect ALL training images (no labels needed) ----------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "def is_image_file(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in IMG_EXTS\n",
    "\n",
    "all_train_imgs = [str(p.resolve()) for p in TRAIN_DIR.rglob(\"*\") if is_image_file(p)]\n",
    "if not all_train_imgs:\n",
    "    raise SystemExit(f\"No images found under {TRAIN_DIR}. Check your zip contents.\")\n",
    "\n",
    "random.shuffle(all_train_imgs)\n",
    "n = len(all_train_imgs); n_tr = int(0.9*n)\n",
    "train_imgs = all_train_imgs[:n_tr]\n",
    "val_imgs   = all_train_imgs[n_tr:]\n",
    "print(f\"Train/Val (ALL training_dataset/ images): train={len(train_imgs)}  val={len(val_imgs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f651a-3ff8-4963-9913-20749c986849",
   "metadata": {},
   "source": [
    "## Step 6: Get train_loader and val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "733da491-a38f-4a6e-8de7-525db2b47499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Transforms / Datasets / Loaders (single-process, spawn-proof) ----------\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torch, gc\n",
    "\n",
    "# --- HARD RESET: kill any old loaders/workers lingering in memory ---\n",
    "for _name in [\"train_loader\", \"val_loader\"]:\n",
    "    try:\n",
    "        del globals()[_name]\n",
    "    except KeyError:\n",
    "        pass\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def fullframe_transform(img_size=IMG_SIZE, grayscale=GRAYSCALE):\n",
    "    t = [transforms.Resize((img_size, img_size), interpolation=InterpolationMode.BILINEAR)]\n",
    "    if grayscale:\n",
    "        t.append(transforms.Grayscale(1))\n",
    "    t.append(transforms.ToTensor())  # [0,1]\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "FF_TF = fullframe_transform()\n",
    "\n",
    "class FullFrameDS(Dataset):\n",
    "    def __init__(self, img_paths):\n",
    "        self.paths = img_paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.paths[i]\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            x  = FF_TF(im)\n",
    "        return x, 0\n",
    "\n",
    "PIN = (DEVICE == \"cuda\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    FullFrameDS(train_imgs),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,          # << no worker subprocesses\n",
    "    pin_memory=PIN,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FullFrameDS(val_imgs),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,          # << no worker subprocesses\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Quick sanity: ensure single-process\n",
    "assert train_loader.num_workers == 0 and val_loader.num_workers == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b3462-836c-4c8d-8e2f-b577af9b71a6",
   "metadata": {},
   "source": [
    "## Step 7: Define Autoencoder and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05f1b703-6c3a-4e73-9a6e-dfcf19f1218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 7 — Define Autoencoder and evaluation functions (TorchScript-friendly) =====\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- UNet-lite Blocks ----------\n",
    "class DWSeparable(nn.Module):\n",
    "    \"\"\"Depthwise conv (3x3) + pointwise conv (1x1) + BN + GELU.\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=stride, padding=1, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.bn(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class Conv1x1(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.c = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.b = nn.BatchNorm2d(out_ch)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.gelu(self.b(self.c(x)))\n",
    "\n",
    "# ---------- Encoder ----------\n",
    "class Enc(nn.Module):\n",
    "    def __init__(self, in_ch: int, latent_dim: int, img_size: int = IMG_SIZE):\n",
    "        super().__init__()\n",
    "        # 256 -> 128 -> 64 -> 32 -> 16\n",
    "        self.stem  = DWSeparable(in_ch, 32, stride=2)     # 32 x 128 x 128\n",
    "        self.b1    = DWSeparable(32, 64, stride=2)        # 64 x 64 x 64\n",
    "        self.b2    = DWSeparable(64, 96, stride=2)        # 96 x 32 x 32\n",
    "        self.b3    = DWSeparable(96, 96, stride=2)        # 96 x 16 x 16\n",
    "\n",
    "        self._feat_shape = (96, img_size // 16, img_size // 16)  # (C, H, W)\n",
    "        c, h, w = self._feat_shape\n",
    "        self.fc = nn.Linear(c * h * w, latent_dim)\n",
    "        self.latent_dim = latent_dim  # <-- grader can find this\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        s0 = self.stem(x)   # 32x128x128\n",
    "        s1 = self.b1(s0)    # 64x64x64\n",
    "        s2 = self.b2(s1)    # 96x32x32\n",
    "        s3 = self.b3(s2)    # 96x16x16\n",
    "        c, h, w = self._feat_shape\n",
    "        z = self.fc(s3.view(s3.size(0), c * h * w))\n",
    "        return z, (s0, s1, s2, s3)\n",
    "\n",
    "# ---------- Decoder ----------\n",
    "class Dec(nn.Module):\n",
    "    def __init__(self, out_ch: int, latent_dim: int, img_size: int = IMG_SIZE):\n",
    "        super().__init__()\n",
    "        # latent maps back to 96 x 16 x 16\n",
    "        self._feat_shape = (96, img_size // 16, img_size // 16)\n",
    "        c, H, W = self._feat_shape\n",
    "        self.fc = nn.Linear(latent_dim, c * H * W)\n",
    "\n",
    "        # match skip channels to h BEFORE each block (add, then DWSeparable may change channels)\n",
    "        self.match3 = Conv1x1(96, 96)  # s3: 96x16x16 -> 96  (16x16 stage)\n",
    "        self.match2 = Conv1x1(96, 96)  # s2: 96x32x32 -> 96  (32x32 stage)\n",
    "        self.match1 = Conv1x1(64, 64)  # s1: 64x64x64 -> 64  (64x64 stage)\n",
    "        self.match0 = Conv1x1(32, 48)  # s0: 32x128x128 -> 48 (128x128 stage)\n",
    "\n",
    "        self.u3 = DWSeparable(96, 96)  # at 16x16\n",
    "        self.u2 = DWSeparable(96, 64)  # at 32x32\n",
    "        self.u1 = DWSeparable(64, 48)  # at 64x64\n",
    "        self.u0 = DWSeparable(48, 32)  # at 128x128\n",
    "\n",
    "        self.head = nn.Conv2d(32, out_ch, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        skips: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        s0, s1, s2, s3 = skips  # s0:128x128x32, s1:64x64x64, s2:32x32x96, s3:16x16x96\n",
    "        c, H, W = self._feat_shape\n",
    "\n",
    "        # map z -> 96x16x16\n",
    "        h = self.fc(z).view(z.size(0), c, H, W)\n",
    "\n",
    "        # 16x16 stage\n",
    "        h = self.u3(h + self.match3(s3))\n",
    "\n",
    "        # 32x32 stage\n",
    "        h = F.interpolate(h, scale_factor=2.0, mode='nearest')\n",
    "        h = self.u2(h + self.match2(s2))\n",
    "\n",
    "        # 64x64 stage\n",
    "        h = F.interpolate(h, scale_factor=2.0, mode='nearest')\n",
    "        h = self.u1(h + self.match1(s1))\n",
    "\n",
    "        # 128x128 stage\n",
    "        h = F.interpolate(h, scale_factor=2.0, mode='nearest')\n",
    "        h = self.u0(h + self.match0(s0))\n",
    "\n",
    "        # 256x256 final\n",
    "        h = F.interpolate(h, scale_factor=2.0, mode='nearest')\n",
    "        return torch.sigmoid(self.head(h))\n",
    "\n",
    "# ---------- Autoencoder wrapper ----------\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, c: int, latent_dim: int, img_size: int = IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.enc = Enc(c, latent_dim, img_size)\n",
    "        self.dec = Dec(c, latent_dim, img_size)\n",
    "        self.latent_dim = latent_dim   # <-- top-level for grader\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z, skips = self.enc(x)\n",
    "        return self.dec(z, skips)\n",
    "\n",
    "# Instantiate, optimizer, losses, eval util\n",
    "CHANNELS = 1 if GRAYSCALE else 3\n",
    "model = AE(CHANNELS, LATENT_DIM, IMG_SIZE).to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "crit_mse = nn.MSELoss()\n",
    "crit_l1  = nn.L1Loss()\n",
    "\n",
    "def recon_loss(y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    if LAMBDA_L1 <= 0:\n",
    "        return crit_mse(y, x)\n",
    "    return (1 - LAMBDA_L1) * crit_mse(y, x) + LAMBDA_L1 * crit_l1(y, x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_full_mse(model: nn.Module, loader) -> float:\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = model(x)\n",
    "        total += crit_mse(y, x).item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "    return total / max(1, n)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "ckpt_path = \"model_ts.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54717aec-b85b-42bc-94a5-fa13ec4ed109",
   "metadata": {},
   "source": [
    "## Step 8: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "728425fd-349a-491b-8d4f-9d28e28c87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ee0705e14545a1939ffe86e289c828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 1/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.011027 | val_mse=0.001567 | time=155.2s\n",
      "  ↳ New best! val_mse=0.001567 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644aadf9134b47fa8d47a6c5764a6263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 2/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | train_loss=0.010102 | val_mse=0.001494 | time=153.0s\n",
      "  ↳ New best! val_mse=0.001494 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573445c968dc4df2b1ef169521f77ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 3/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | train_loss=0.010152 | val_mse=0.001466 | time=148.3s\n",
      "  ↳ New best! val_mse=0.001466 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ac69e18c5745a7a2a88e19cb8b0ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 4/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 | train_loss=0.009935 | val_mse=0.001423 | time=148.3s\n",
      "  ↳ New best! val_mse=0.001423 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4733e09413c84d3fb4d44cece2d60cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 5/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | train_loss=0.009770 | val_mse=0.001426 | time=157.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12898537a174983bb0414a44a66dbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 6/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 | train_loss=0.009314 | val_mse=0.001405 | time=156.9s\n",
      "  ↳ New best! val_mse=0.001405 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b5b1d0c4914968a5c7b503d04fd293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 7/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 | train_loss=0.009315 | val_mse=0.001422 | time=157.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3577a6dcb5e411786a12c5bd537b44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 8/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 | train_loss=0.009149 | val_mse=0.001383 | time=157.8s\n",
      "  ↳ New best! val_mse=0.001383 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe186dbc9e64920aa34d0e2d5a084b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 9/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 | train_loss=0.008709 | val_mse=0.001375 | time=156.9s\n",
      "  ↳ New best! val_mse=0.001375 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f2849a60d431195ee4ae9296eb600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 10/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | train_loss=0.008586 | val_mse=0.001387 | time=148.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e150a0832b47bea05163f6c75efe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 11/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 | train_loss=0.008478 | val_mse=0.001359 | time=148.1s\n",
      "  ↳ New best! val_mse=0.001359 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e828db3b67d4b9a857c27448c2c0892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 12/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 | train_loss=0.008458 | val_mse=0.001361 | time=156.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5aa692b3244b76ae8fd4a403fb1da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 13/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 | train_loss=0.008254 | val_mse=0.001360 | time=157.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5823542345a647a986de49f70d210b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 14/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 | train_loss=0.008157 | val_mse=0.001339 | time=157.0s\n",
      "  ↳ New best! val_mse=0.001339 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e29c850d0404fe2b4a7a47d734933ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 15/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 | train_loss=0.008232 | val_mse=0.001344 | time=157.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299073ecb5b14504bf9b267e4257e0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 16/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 | train_loss=0.008154 | val_mse=0.001381 | time=157.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc678bb4a374f05961ded6dd16bebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 17/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 | train_loss=0.008070 | val_mse=0.001326 | time=157.2s\n",
      "  ↳ New best! val_mse=0.001326 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32640dfe0ae44c89b3df15d02b9694d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 18/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 | train_loss=0.008017 | val_mse=0.001327 | time=156.3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d22fae312164bd0bd28c3177eaf137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 19/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 | train_loss=0.008090 | val_mse=0.001323 | time=156.3s\n",
      "  ↳ New best! val_mse=0.001323 — saved TorchScript to model_ts.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2ab4db278a4dd0b66c72c10ad2db4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 20/20:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | train_loss=0.008011 | val_mse=0.001320 | time=158.0s\n",
      "  ↳ New best! val_mse=0.001320 — saved TorchScript to model_ts.pt\n"
     ]
    }
   ],
   "source": [
    "# ===== Training loop (console tqdm + epoch timing) =====\n",
    "import time\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # auto picks notebook/console; falls back gracefully if ipywidgets missing\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "ckpt_path = \"model_ts.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Train {epoch}/{EPOCHS}\",\n",
    "        leave=False,\n",
    "        dynamic_ncols=True,\n",
    "        mininterval=0.5,\n",
    "    )\n",
    "\n",
    "    for xb, _ in pbar:\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = model(xb)\n",
    "        loss = recon_loss(yb, xb)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "        seen    += xb.size(0)\n",
    "        pbar.set_postfix(loss=running / max(1, seen))\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # ---- validation ----\n",
    "    val_mse = eval_full_mse(model, val_loader)\n",
    "    train_mse = running / max(1, seen)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # clean one-liner per epoch (no tqdm.write)\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_mse:.6f} | val_mse={val_mse:.6f} | time={epoch_time:.1f}s\")\n",
    "\n",
    "    # ---- save best + TorchScript ----\n",
    "    if val_mse < best_val:\n",
    "        best_val = val_mse\n",
    "        try:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                scripted = torch.jit.script(model.cpu())\n",
    "                scripted.save(ckpt_path)\n",
    "            model.to(DEVICE)\n",
    "            print(f\"  ↳ New best! val_mse={best_val:.6f} — saved TorchScript to {ckpt_path}\")\n",
    "        except Exception as e:\n",
    "            # If scripting fails mid-training, don’t crash the run\n",
    "            print(f\"  ⚠️ TorchScript export failed this epoch: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af9ecc-38dc-441e-8751-dd91b5289399",
   "metadata": {},
   "source": [
    "## Step 9: Check Error on training dataset provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b850c28d-0442-4ce6-87b6-9bf8cf45a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_loader = DataLoader(\n",
    "    FullFrameDS(train_imgs + val_imgs),  # entire training_dataset/\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=nw,\n",
    "    pin_memory=(DEVICE==\"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50fa9f-9852-41f4-aaa1-7cb4a073962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rebuild all-train eval loader (single-process, spawn-proof) ----\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# pick which paths you want in \"all\": here I use your full training set\n",
    "all_imgs = train_imgs  # or train_imgs + val_imgs if you truly want both\n",
    "\n",
    "all_ds = FullFrameDS(all_imgs)\n",
    "\n",
    "# Slightly larger batch for faster eval but still safe\n",
    "EVAL_BATCH = min(4 * BATCH_SIZE, 64)\n",
    "\n",
    "# Nuke any old loader\n",
    "try:\n",
    "    del all_train_loader\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "all_train_loader = DataLoader(\n",
    "    all_ds,\n",
    "    batch_size=EVAL_BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=0,      # << critical: no workers → no spawn/pickle\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# ---- Load scripted model and evaluate ----\n",
    "model = torch.jit.load(ckpt_path, map_location=DEVICE)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_full_mse = eval_full_mse(model, all_train_loader)\n",
    "\n",
    "print(f\"Full-MSE on all-train set: {all_full_mse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c0dc6-6af5-4847-85fa-17af8dba69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(ckpt_path, map_location=DEVICE)\n",
    "model.eval()\n",
    "all_full_mse   = eval_full_mse(model, all_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fb730-8621-4c34-bf56-06c3f5e6d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Full-image MSE on training data ===\")\n",
    "print(f\"Entire training_dataset: {all_full_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e6a7-225a-451b-80fe-7eb91f124953",
   "metadata": {},
   "source": [
    "## Step 10: Submit to Server and Also Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c24c09-c2d2-4bc6-816b-3ef9c8678736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ts.pt 6.76655 MB\n",
      "model_scripted.pt 0.721842 MB\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "for f in [\"model_ts.pt\",\"model_scripted.pt\",\"TinyDemiUNet_scripted.pt\",\"Model_B.pth\"]:\n",
    "    if os.path.exists(f):\n",
    "        print(f, os.path.getsize(f)/1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef9571b-7143-43a4-b77b-12bb0b931ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission received for team 'Charles Oliveira'. Attempt #2.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Submit to server\n",
    "# -------------------------\n",
    "import requests\n",
    "def submit_model(token: str, model_path: str, server_url=\"http://hadi.cs.virginia.edu:9000\"):\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        files = {\"file\": f}\n",
    "        data = {\"token\": token}\n",
    "        response = requests.post(f\"{server_url}/submit\", data=data, files=files)\n",
    "        resp_json = response.json()\n",
    "        if \"message\" in resp_json:\n",
    "            print(f\"✅ {resp_json['message']}\")\n",
    "        else:\n",
    "            print(f\"❌ Submission failed: {resp_json.get('error')}\")\n",
    "\n",
    "\n",
    "# Replace with your team token\n",
    "my_token = \"2d99218bc499d7c8b376acc1bb545884\"\n",
    "file_name = \"model_ts.pt\"\n",
    "submit_model(my_token, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb7a36c-96bc-4e0e-93f5-2d98cd350e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Model size=6.4531, Submitted at=Nov 11, 2025 10:37:38 PM, Status=pending\n",
      "Attempt 2: Model size=6.4531, Submitted at=Nov 11, 2025 11:54:25 PM, Status=pending\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "#  Check status\n",
    "# -------------------------\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def check_submission_status(my_token, max_retries=3):\n",
    "    url = f\"http://hadi.cs.virginia.edu:9000/submission-status/{my_token}\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            attempts = response.json()\n",
    "            for a in attempts:\n",
    "                model_size = f\"{a['model_size']:.4f}\" if isinstance(a['model_size'], (float, int)) else \"None\"\n",
    "\n",
    "                print(f\"Attempt {a['attempt']}: Model size={model_size}, \"\n",
    "                      f\"Submitted at={a['submitted_at']}, Status={a['status']}\")\n",
    "\n",
    "            if attempts and attempts[-1]['status'].lower() == \"broken file\":\n",
    "                print(\"⚠️ Your model on the server is broken!\")\n",
    "            return  # success, exit function\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            # Server says rate limit exceeded\n",
    "            try:\n",
    "                error_json = response.json()\n",
    "                wait_seconds = int(error_json.get(\"error\", \"\").split()[-2])\n",
    "            except Exception:\n",
    "                wait_seconds = 15  # default fallback\n",
    "            print(f\"⏳ Rate limited. Waiting {wait_seconds} seconds before retry...\")\n",
    "            time.sleep(wait_seconds + 1)  # wait a bit longer to be safe\n",
    "\n",
    "        else:\n",
    "            print(f\"❌ Error {response.status_code}: {response.text}\")\n",
    "            return\n",
    "\n",
    "    print(\"⚠️ Max retries reached. Try again later.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "check_submission_status(my_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a567917-640e-4612-8e22-c73313346cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d53bb7-02dc-4ac4-9a68-61b7c6effa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
